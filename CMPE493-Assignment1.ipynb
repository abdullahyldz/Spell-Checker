{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def editDistance(word1, word2):\n",
    "    len1 = len(word1)\n",
    "    len2 = len(word2)\n",
    "    distanceMap = [[0] for i in range(len1+1)]\n",
    "    distanceMap[0] = [i for i in range(len2+1)]\n",
    "    for i in range(len1+1):\n",
    "        distanceMap[i][0] = i\n",
    "    \n",
    "    for i in range(len1):\n",
    "        for j in range(len2):\n",
    "            adder = 0 if(word1[i]==word2[j]) else 1\n",
    "            dist = min(distanceMap[i+1][j]+1,distanceMap[i][j+1]+1,distanceMap[i][j]+adder)\n",
    "            distanceMap[i+1].append(dist)\n",
    "    \n",
    "    return distanceMap[len1][len2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D_L_editDistance(word1, word2):\n",
    "    len1 = len(word1)\n",
    "    len2 = len(word2)\n",
    "    distanceMap = [[0] for i in range(len1+1)]\n",
    "    distanceMap[0] = [i for i in range(len2+1)]\n",
    "    for i in range(len1+1):\n",
    "        distanceMap[i][0] = i\n",
    "    \n",
    "    for i in range(1,len1+1):\n",
    "        for j in range(1,len2+1):\n",
    "            adder = 0 if(word1[i-1]==word2[j-1]) else 1\n",
    "            #print(i,j,adder)\n",
    "            dist = min(distanceMap[i][j-1]+1,distanceMap[i-1][j]+1,distanceMap[i-1][j-1]+adder)\n",
    "            if(i>1 and j>1 and word1[i-2:i] == word2[j-2:j][::-1]):\n",
    "                dist = min(distanceMap[i-2][j-2]+1, dist)\n",
    "            distanceMap[i].append(dist)\n",
    "    #print(distanceMap)\n",
    "    return distanceMap[len1][len2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D_L_Backtrack(word1, word2):\n",
    "    len1 = len(word1)\n",
    "    len2 = len(word2)\n",
    "    if(word1==''):\n",
    "        return [('insertion', word2, '@')]\n",
    "    elif(word2 == ''):\n",
    "        return [('deletion', word1, '@')]\n",
    "    distanceMap = [[0] for i in range(len1+1)]\n",
    "    distanceMap[0] = [i for i in range(len2+1)]\n",
    "    for i in range(len1+1):\n",
    "        distanceMap[i][0] = i\n",
    "    \n",
    "    for i in range(1,len1+1):\n",
    "        for j in range(1,len2+1):\n",
    "            adder = 0 if(word1[i-1]==word2[j-1]) else 1\n",
    "            #print(i,j,adder)\n",
    "            dist = min(distanceMap[i][j-1]+1,distanceMap[i-1][j]+1,distanceMap[i-1][j-1]+adder)\n",
    "            if(i>1 and j>1 and word1[i-2:i] == word2[j-2:j][::-1]):\n",
    "                dist = min(distanceMap[i-2][j-2]+1, dist)\n",
    "            distanceMap[i].append(dist)\n",
    "            \n",
    "    i = len1\n",
    "    j = len2\n",
    "    operations = []\n",
    "    while(i>=1 and j>=1):\n",
    "        if(distanceMap[i][j] == distanceMap[i][j-1]+1):\n",
    "            #print('insertion of %s after %s' % (word2[j-1], word1[i-1]))\n",
    "            operations.append(('insertion', word2[j-1], word1[i-1]))\n",
    "            j=j-1\n",
    "        elif(distanceMap[i][j] == distanceMap[i-1][j]+1):\n",
    "            #print('deletion of %s after %s' % (word1[i-1], word1[i-2]))\n",
    "            operations.append(('deletion', word1[i-1], word1[i-2]))\n",
    "            i=i-1\n",
    "        elif(distanceMap[i][j] == distanceMap[i-1][j-1] + (0 if word1[i-1] == word2[j-1] else 1)):\n",
    "            if(not word1[i-1] == word2[j-1]):    \n",
    "                #print('substitution of %s with %s' % (word2[j-1], word1[i-1]))\n",
    "                operations.append(('substitution', word2[j-1], word1[i-1]))\n",
    "            i=i-1\n",
    "            j=j-1\n",
    "        else:\n",
    "            #print('transposition of %s with %s' % (word2[j-1], word1[i-1]))\n",
    "            operations.append(('transposition', word2[j-1], word1[i-1]))\n",
    "            i=i-2\n",
    "            j=j-2\n",
    "    word1=word1[:-1]+'@'\n",
    "    word2=word2[:-1]+'@'\n",
    "    while(i>0):\n",
    "        #print('deletion of %s after %s' % (word1[i-1], word1[i-2]))\n",
    "        operations.append(('deletion', word1[i-1], word1[i-2]))\n",
    "        i=i-1\n",
    "    while(j>0):\n",
    "        #print('insertion of %s after %s' % (word2[j-1], word1[i-1]))\n",
    "        operations.append(('insertion', word2[j-1], word1[i-1]))\n",
    "        j=j-1\n",
    "    return operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenMe(inputString):\n",
    "    inputString = inputString.lower()\n",
    "    outputString = ''\n",
    "    for ch in inputString.replace('--',' ').replace('\\'', ''):\n",
    "        if(ch.isalpha() or ch == ' ' or ch.isnumeric()):\n",
    "            outputString+=ch\n",
    "        else:\n",
    "            outputString+=' '\n",
    "    return outputString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('corpusMini.txt').read().lower()\n",
    "corpus2 = tokenMe(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words_correct = []\n",
    "with open('test-words-correct.txt') as fp:\n",
    "    line = fp.readline()\n",
    "    while(line):\n",
    "        line = tokenMe(line)\n",
    "        test_words_correct.append(line[:-1])\n",
    "        line = fp.readline()\n",
    "test_words_misspelled = []\n",
    "with open('test-words-misspelled.txt') as fp2:\n",
    "    line = fp2.readline()\n",
    "    while(line):\n",
    "        line = tokenMe(line)\n",
    "        test_words_misspelled.append(line[:-1])\n",
    "        line = fp2.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell_errors = []\n",
    "with open('spell-errors.txt') as fp:\n",
    "    line = fp.readline()\n",
    "    while(line):\n",
    "        spell_errors.append(line[:-1])\n",
    "        line = fp.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell_error_samples = {}\n",
    "for spl_error in spell_errors:\n",
    "    spl = spl_error.split(':')\n",
    "    spell_error_samples[spl[0].lower()] = []\n",
    "    splErrors = spl[1].split(',')\n",
    "    for err in splErrors:\n",
    "        spell_error_samples[spl[0].lower()].append(err.replace(' ', '').lower())\n",
    "for se in spell_error_samples.items():\n",
    "    spell_error_samples[se[0]] = []\n",
    "    for err in se[1]:\n",
    "        if('*' in err):\n",
    "            sp = err.split('*')\n",
    "            spell_error_samples[se[0]].append((sp[0], int(sp[1])))\n",
    "        else:\n",
    "            spell_error_samples[se[0]].append((err,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_position(letter):\n",
    "    return ord(letter) - 97\n",
    "letters    = 'abcdefghijklmnopqrstuvwxyz@*'\n",
    "\n",
    "def createConfusionMatricesFromCorrections():\n",
    "    confusion_matrix_substitution = [[0 for i in range(len(letters))] for j in range(len(letters))]\n",
    "    confusion_matrix_transposition = [[0 for i in range(len(letters))] for j in range(len(letters))]\n",
    "    confusion_matrix_insertion = [[0 for i in range(len(letters))] for j in range(len(letters))]\n",
    "    confusion_matrix_deletion = [[0 for i in range(len(letters))] for j in range(len(letters))]\n",
    "    confusion_matrix = {'deletion': confusion_matrix_deletion,\n",
    "                       'insertion': confusion_matrix_insertion,\n",
    "                       'substitution': confusion_matrix_substitution,\n",
    "                       'transposition': confusion_matrix_transposition}\n",
    "    for (key,spellErrors) in spell_error_samples.items():\n",
    "        for spellError in spellErrors:\n",
    "            corrections = D_L_Backtrack(spellError[0], key)\n",
    "            for correction in corrections:\n",
    "                    x = char_position(correction[1]) if 0<=char_position(correction[1])<=25 else 27\n",
    "                    y = char_position(correction[2]) if correction[2] != '@' and 0<=char_position(correction[2])<=25 else 26 if correction[2] == '@' else 27\n",
    "                    confusion_matrix[correction[0]][x][y]+=spellError[1]\n",
    "\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = createConfusionMatricesFromCorrections()\n",
    "deletionFrame = pd.DataFrame(confusion_matrix['deletion'], columns = list('abcdefghijklmnopqrstuvwxyz@*'), index = list(letters))\n",
    "insertionFrame= pd.DataFrame(confusion_matrix['insertion'], columns = list('abcdefghijklmnopqrstuvwxyz@*'), index = list(letters))\n",
    "substitutionFrame=pd.DataFrame(confusion_matrix['substitution'], columns = list('abcdefghijklmnopqrstuvwxyz@*'), index = list(letters))\n",
    "transpositionFrame=pd.DataFrame(confusion_matrix['transposition'], columns = list('abcdefghijklmnopqrstuvwxyz@*'), index = list(letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     a    b    c    d    e    f    g    h    i   j ...    s     t    u    v  \\\n",
      "a   38   58   98   69  644   37   69   59  294   1 ...  101   237  174   21   \n",
      "b   31   20    0   14   14    3    4    1    5   0 ...    3    18   21    2   \n",
      "c  180    8  208   12  183   42   17   18  139   0 ...  358    85   43    7   \n",
      "d   73   20    7   67  398    3   32   60   46   1 ...   56   245   26    7   \n",
      "e  321  110  139  219  280   87  195  223  354   6 ...  513  1078  166  128   \n",
      "f   16    4    7    2   32  137    2    6   17   0 ...    7    12    2   16   \n",
      "g   55   10   13   47   79    5   81    9  121   7 ...   18    30   29    3   \n",
      "h   30    2  312   16   61  148   57    1   67   1 ...  129   111   28   18   \n",
      "i  481   32  256   88  487   96   61   73  141   1 ...  337   291  203   35   \n",
      "j    0    0    0    0   17    0    4    0    0   0 ...    0     0    0    0   \n",
      "k    7    0   54    2    1    0    8    1    1   0 ...   17     4    2    0   \n",
      "l  102   31   34   35  213   32   24   39   81   0 ...   39   128   48    6   \n",
      "m   37    0    6   11   69    2    2   10   37   0 ...   12     8   31    1   \n",
      "n  293    1   23   48  439    8   36   45  229   0 ...   75   120  105   42   \n",
      "o  141   19  123   15  243   23   21   65  177  15 ...   81    98   64   12   \n",
      "p   20   10    8    2   26    5    2    4   18   0 ...   44    21    8    0   \n",
      "q    2    0    6    0    9    0    5    0    1   0 ...    7     0    1    0   \n",
      "r  233   39   65   47  344  100   58   46   75   1 ...   74   184  177    9   \n",
      "s  119    7  130   36  403   10   25   39  167   0 ...  321   209   61    3   \n",
      "t  117   12   87   89  237   44   26   71  166   1 ...  216   226   45    6   \n",
      "u  258   27   56   29  315   38  213   33  104   4 ...   78    97   13    4   \n",
      "v    9    3    1    8    7    6    0    0   12   0 ...    4    17    8    0   \n",
      "w   10    0    5    1   16    0    0    2    5   0 ...   45    11    7    2   \n",
      "x    4    0    1    0   13    0    0    0    1   0 ...    1     0    0    0   \n",
      "y   33    4    8   26  206    5   28   15   26   0 ...   52    60    6    1   \n",
      "z    0    0    1    0    7    0    1    0    5   0 ...    8     0    1    0   \n",
      "@    0    0    0    0    0    0    0    0    0   0 ...    0     0    0    0   \n",
      "*   16    1    2   18   50    3    3    4   24   0 ...   14    70   11    0   \n",
      "\n",
      "     w   x    y   z   @  *  \n",
      "a   13   9   24   1  59  2  \n",
      "b    0   0    1   0   9  0  \n",
      "c    5  46   31   1  13  0  \n",
      "d   11   2   31   2   6  1  \n",
      "e   51  12  207   6  37  6  \n",
      "f    0   2    3   0   9  0  \n",
      "g   16   3   37   3   7  0  \n",
      "h  130  27   21   0  45  1  \n",
      "i   15  22   37   8  14  3  \n",
      "j    0   0    0   0   0  0  \n",
      "k    3   1    0   0  48  1  \n",
      "l   12   2   13   1   3  4  \n",
      "m    4   1   10   1  18  0  \n",
      "n    8   1   36   1  18  1  \n",
      "o    7   1   15   0  17  0  \n",
      "p    1   2    4   0  85  0  \n",
      "q    4   2    0   0   0  0  \n",
      "r   18   1   42   1  19  0  \n",
      "s    8  36   79  12  84  3  \n",
      "t    2  15   19   0  19  3  \n",
      "u   16   5   25   0  11  1  \n",
      "v    1   0    1   0   4  1  \n",
      "w    0   0    0   0  60  0  \n",
      "x    0   0    0   0   2  0  \n",
      "y    1   1    0   0   4  0  \n",
      "z    0   0    0   4   0  0  \n",
      "@    0   0    0   0   0  0  \n",
      "*    4   1   19   0   7  7  \n",
      "\n",
      "[28 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "print(insertionFrame);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list=re.split('\\s+',corpus2)\n",
    "word_list_set = set(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 3383\n"
     ]
    }
   ],
   "source": [
    "numTokens = len(word_list)\n",
    "print(\"Number of tokens: %d\" %numTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordFreqDf = pd.DataFrame({'word':[],'freq':[],'percentage':[]})\n",
    "wordFreq = []\n",
    "for word in word_list_set:\n",
    "    count = word_list.count(word)\n",
    "    wordFreq.append([word, count, round(count/numTokens,5)])\n",
    "wordFreq = sorted(wordFreq, key=lambda tup: tup[2], reverse=True)\n",
    "wordFreqDf = pd.DataFrame(wordFreq, columns=['word','count','percentage'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getWordFreq: Returns the frequency of occurrence of a word in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordFreq(word):\n",
    "    if(word not in word_list_set):\n",
    "        return 0\n",
    "    located = wordFreqDf.loc[wordFreqDf['word'] == word]\n",
    "    return float(located['percentage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04789"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = getWordFreq('the')\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProbabilityFromConfusionMatrix(operation):\n",
    "    if(operation[0] == 'insertion'):\n",
    "        series = insertionFrame.loc[operation[1]]\n",
    "        return round(series[operation[2]]/sum(series), 7)\n",
    "    elif(operation[0] == 'deletion'):\n",
    "        series = deletionFrame.loc[operation[1]]\n",
    "        return round(series[operation[2]]/sum(series), 7)\n",
    "\n",
    "    elif(operation[0] == 'substitution'):\n",
    "        series = substitutionFrame.loc[operation[1]]\n",
    "        return round(series[operation[2]]/sum(series), 7)\n",
    "\n",
    "        return substitutionFrame.loc[operation[1]][operation[2]]\n",
    "    elif(operation[0] == 'transposition'):\n",
    "        series = transpositionFrame.loc[operation[1]]\n",
    "        return round(series[operation[2]]/sum(series), 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getCandidates: returns the candidate spell checkings of a word ( edit distance is 1 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCandidates(word):\n",
    "    candidates_len1 = [word for word in word_list_set if(abs(len(word)-len(testWord))<=1)]\n",
    "    candidates_edit1 = [candidate for candidate in candidates_len1 if(editDistance(candidate, testWord)==1)]\n",
    "    return candidates_edit1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['end', 'and', 'no']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testWord = 'nd'\n",
    "cands = getCandidates(testWord)\n",
    "cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test word: ', testWord)\n",
    "print()\n",
    "best = 0\n",
    "for cand in cands:\n",
    "    print('cand: ', cand)\n",
    "    px = getWordFreq(cand)\n",
    "    print('freq\\t\\t: ', px)\n",
    "    operation = D_L_Backtrack(testWord, cand)\n",
    "    print('operation: ', operation)\n",
    "    pcm = getProbabilityFromConfusionMatrix(operation[0])\n",
    "    print('probability confusion matrix: ', pcm)\n",
    "    print('multiplied: ', pcm*px)\n",
    "    if(pcm*px>best):\n",
    "        best = pcm*px\n",
    "    print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('insertion', 'e', '@')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0056154"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0003, 0.02838, 0.00236]\n"
     ]
    }
   ],
   "source": [
    "print([getWordFreq(cand) for cand in cands])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('deletion', 'e', '@')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_L_Backtrack(cands[0], testWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('deletion', 'ab', '@')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_L_Backtrack('ab', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_L_editDistance('oslo', 'snow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_L_Backtrack('oslo', 'snow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
